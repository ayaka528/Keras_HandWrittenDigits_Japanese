{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to return an array of file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def return_list_of_files(rootdir, printname=False):\n",
    "    all_files = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(subdir, file))\n",
    "            if printname: \n",
    "                print(os.path.join(subdir, file))\n",
    "    return np.asarray(all_files)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to load data from file names into features + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(dataset_path):\n",
    "    images_list = return_list_of_files(dataset_path)\n",
    "    \n",
    "    features = np.ndarray(shape=(len(images_list), 28, 28),\n",
    "                    dtype=np.uint8)\n",
    "    labels = []\n",
    "    for i in range(len(images_list)):\n",
    "        im = mpimg.imread(images_list[i])\n",
    "        \n",
    "        features[i] = im\n",
    "        labels.append(images_list[i].split(\"/\")[1])\n",
    "    return features, np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_data(\"TrainingDataAll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern 1 - Load both train and test datasets (2 data folders)\n",
    "## Step 1: Use the loaded data as train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (64076, 28, 28) \n",
      " (64076,)\n"
     ]
    }
   ],
   "source": [
    "X_train = features\n",
    "Y_train = labels\n",
    "print(\"\\n\", X_train.shape, \"\\n\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load \"Test\" data from file names into features + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test, labels_test = load_data(\"TestData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (102, 28, 28) \n",
      " (102,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", features_test.shape, \"\\n\", labels_test.shape)\n",
    "X_test = features_test\n",
    "Y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape the two-dimentional array (each image information | 28Ã—28 ) to single dimentional array\n",
    "X_train  = X_train.reshape(64076, 784)\n",
    "X_test   = X_test.reshape(102, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern 2 - Split data into train and test datasets (1 data folder)\n",
    "## Step 1: Label encoder to convert string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "labels_encoded = le.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split features and labels into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] splitted train and test data...\n",
      "Train data  : (51260, 28, 28)\n",
      "Test data   : (12816, 28, 28)\n",
      "Train labels: (51260,)\n",
      "Test labels : (12816,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the training and testing data\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(features, labels_encoded, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"[STATUS] splitted train and test data...\")\n",
    "print(\"Train data  : {}\".format(X_train.shape))\n",
    "print(\"Test data   : {}\".format(X_test.shape))\n",
    "print(\"Train labels: {}\".format(Y_train.shape))\n",
    "print(\"Test labels : {}\".format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[3809].reshape(28, 28))\n",
    "print(Y_train[3809])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[100].reshape(28, 28))\n",
    "print(Y_test[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (51260, 28, 28, 1)\n",
      "51260 train samples\n",
      "12816 test samples\n",
      "Train on 51260 samples, validate on 12816 samples\n",
      "Epoch 1/12\n",
      "51260/51260 [==============================] - 70s 1ms/step - loss: 1.4881 - acc: 0.4900 - val_loss: 0.4780 - val_acc: 0.8584\n",
      "Epoch 2/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.4622 - acc: 0.8569 - val_loss: 0.2142 - val_acc: 0.9423\n",
      "Epoch 3/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.3049 - acc: 0.9066 - val_loss: 0.1654 - val_acc: 0.9522\n",
      "Epoch 4/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.2451 - acc: 0.9261 - val_loss: 0.1234 - val_acc: 0.9658\n",
      "Epoch 5/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.2069 - acc: 0.9369 - val_loss: 0.1016 - val_acc: 0.9709\n",
      "Epoch 6/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.1810 - acc: 0.9454 - val_loss: 0.1358 - val_acc: 0.9578\n",
      "Epoch 7/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.1615 - acc: 0.9516 - val_loss: 0.0807 - val_acc: 0.9782\n",
      "Epoch 8/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.1487 - acc: 0.9549 - val_loss: 0.0765 - val_acc: 0.9799\n",
      "Epoch 9/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.1322 - acc: 0.9603 - val_loss: 0.0750 - val_acc: 0.9801\n",
      "Epoch 10/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.1220 - acc: 0.9643 - val_loss: 0.0706 - val_acc: 0.9809\n",
      "Epoch 11/12\n",
      "51260/51260 [==============================] - 69s 1ms/step - loss: 0.1164 - acc: 0.9654 - val_loss: 0.0694 - val_acc: 0.9798\n",
      "Epoch 12/12\n",
      "51260/51260 [==============================] - 70s 1ms/step - loss: 0.1043 - acc: 0.9691 - val_loss: 0.0717 - val_acc: 0.9801\n",
      "Test loss: 0.0717166511922698\n",
      "Test accuracy: 0.9801029962546817\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use model to predict on test dataset and print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1717\n",
      "           1       1.00      0.99      0.99      1187\n",
      "           2       0.99      0.98      0.98      1237\n",
      "           3       0.95      0.99      0.97      1185\n",
      "           4       1.00      0.98      0.99      1479\n",
      "           5       0.99      0.97      0.98      1243\n",
      "           6       1.00      0.98      0.99      1280\n",
      "           7       0.99      0.97      0.98      1262\n",
      "           8       0.95      0.99      0.97      1089\n",
      "           9       0.95      0.99      0.97      1137\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     12816\n",
      "   macro avg       0.98      0.98      0.98     12816\n",
      "weighted avg       0.98      0.98      0.98     12816\n",
      " samples avg       0.98      0.98      0.98     12816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_pred = model.predict_classes(X_test)\n",
    "print(Y_test[12])\n",
    "print(Y_pred[12])\n",
    "Y_pred_ed = keras.utils.to_categorical(Y_pred, 10)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_ed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Keras_Digits_201901181228.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
